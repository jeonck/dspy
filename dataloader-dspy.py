import dspy
import dotenv
import os
import json
import pandas as pd
from typing import List, Dict, Any
from pathlib import Path
import requests
from dspy.utils import download

class DataConfig:
    """ë°ì´í„° ì„¤ì •"""
    DATASETS = {
        'qa': {
            'url': 'https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl',
            'local_path': 'data/qa_dataset.jsonl',
            'desc': 'ê¸°ìˆ  ê´€ë ¨ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ì…‹'
        }
    }

class DataLoaderSignature(dspy.Signature):
    """ë°ì´í„° ë¡œë” ì‹œê·¸ë‹ˆì²˜"""
    dataset_name = dspy.InputField(desc="ë°ì´í„°ì…‹ ì´ë¦„")
    data_path = dspy.InputField(desc="ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    processed_data = dspy.OutputField(desc="ì „ì²˜ë¦¬ëœ ë°ì´í„°")
    statistics = dspy.OutputField(desc="ë°ì´í„° í†µê³„ ì •ë³´")

class CustomDataLoader(dspy.Module):
    """ì»¤ìŠ¤í…€ ï¿½ï¿½ì´í„° ë¡œë”"""
    
    def __init__(self):
        super().__init__()
        self.predictor = dspy.Predict(DataLoaderSignature)
        self.datasets = {}
        self.ensure_data_directory()
    
    def ensure_data_directory(self) -> None:
        """ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±"""
        Path('data').mkdir(exist_ok=True)
    
    def download_dataset(self, dataset_key: str) -> str:
        """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        try:
            config = DataConfig.DATASETS[dataset_key]
            local_path = config['local_path']
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            Path(local_path).parent.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ
            if not Path(local_path).exists() or Path(local_path).stat().st_size == 0:
                print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {dataset_key}")
                
                # requestsë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                response = requests.get(config['url'])
                response.raise_for_status()
                
                # ì‘ë‹µ ë‚´ìš© í™•ì¸
                content = response.text.strip()
                if not content:
                    raise ValueError("ë‹¤ìš´ë¡œë“œëœ ì»¨í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
                # íŒŒì¼ ì €ì¥
                with open(local_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = Path(local_path).stat().st_size
                print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size/1024:.2f} KB")
                
                # íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                with open(local_path, 'r', encoding='utf-8') as f:
                    preview = f.readline().strip()
                print(f"ğŸ‘€ ì²« ë²ˆì§¸ ë¼ì¸ ë¯¸ë¦¬ë³´ê¸°: {preview[:100]}...")
            
            else:
                file_size = Path(local_path).stat().st_size
                print(f"ğŸ“ ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©: {local_path} (í¬ê¸°: {file_size/1024:.2f} KB)")
            
            return local_path
        
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({dataset_key}): {str(e)}")
            # ì‹¤íŒ¨í•œ ê²½ìš° íŒŒì¼ ì‚­ì œ
            if Path(local_path).exists():
                Path(local_path).unlink()
            raise
    
    def load_jsonl(self, file_path: str) -> List[Dict[str, Any]]:
        """JSONL íŒŒì¼ ë¡œë“œ"""
        try:
            data = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f, 1):
                    try:
                        line = line.strip()
                        if line:  # ë¹ˆ ë¼ì¸ ë¬´ì‹œ
                            data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ ë¼ì¸ {i} JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                        print(f"ë¬¸ì œì˜ ë¼ì¸: {line[:100]}...")
                        continue
            
            if not data:
                # íŒŒì¼ ë‚´ìš© í™•ì¸
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"ğŸ“„ íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
                print(content[:500])
                raise ValueError("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            print(f"\nâœ… ì´ {len(data)}ê°œì˜ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")
            print(f"ğŸ“ ì²« ë²ˆì§¸ í•­ëª© ë¯¸ë¦¬ë³´ê¸°:")
            print(json.dumps(data[0], indent=2, ensure_ascii=False)[:200])
            
            return data
        
        except Exception as e:
            print(f"âŒ JSONL íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {str(e)}")
            raise
    
    def preprocess_data(self, data: List[Dict], dataset_key: str) -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        df = pd.DataFrame(data)
        
        # ë°ì´í„°ì…‹ë³„ ì „ì²˜ë¦¬ ë¡œì§
        if dataset_key == 'qa':
            df = self.preprocess_qa_dataset(df)
        elif dataset_key == 'wiki':
            df = self.preprocess_wiki_dataset(df)
        
        return df
    
    def preprocess_qa_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        """QA ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"""
        try:
            # ë°ì´í„° êµ¬ì¡° í™•ì¸ ë° ë¡œê¹…
            print("\nğŸ“Š ë°ì´í„° ì»¬ëŸ¼:", df.columns.tolist())
            
            # í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ êµ¬ì¡° ìƒì„±
            if 'text' not in df.columns:
                # ë°ì´í„°í”„ë ˆì„ì˜ ì²« ë²ˆì§¸ í–‰ ì¶œë ¥í•˜ì—¬ êµ¬ì¡° í™•ì¸
                print("\nğŸ” ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ:")
                print(df.iloc[0] if not df.empty else "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
                # ë°ì´í„°ê°€ ë‹¨ì¼ í…ìŠ¤ ì»¬ëŸ¼ìœ¼ë¡œ ë˜ì–´ìˆë‹¤ë©´ 'text' ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
                if len(df.columns) == 1:
                    df = df.rename(columns={df.columns[0]: 'text'})
                else:
                    # ë˜ëŠ” ëª¨ë“  ì»¬ëŸ¼ì„ í•©ì³ì„œ text ì»¬ëŸ¼ ìƒì„±
                    df['text'] = df.apply(lambda row: str(row.to_dict()), axis=1)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            df['text'] = df['text'].astype(str).str[:6000]
            
            # ì¤‘ë³µ ì œê±°
            df = df.drop_duplicates(subset=['text'])
            
            return df
        
        except Exception as e:
            print(f"âŒ QA ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            print(f"í˜„ì¬ ë°ì´í„°í”„ë ˆì„ ì •ë³´:")
            print(df.info())
            raise
    
    def preprocess_wiki_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        """Wikipedia ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"""
        # ì˜ˆì‹œ ì „ì²˜ë¦¬ ë¡œì§
        df = df.dropna()
        return df
    
    def calculate_statistics(self, df: pd.DataFrame, dataset_key: str) -> Dict:
        """ë°ì´í„° í†µï¿½ï¿½ ê³„ì‚°"""
        stats = {
            'dataset_name': dataset_key,
            'total_rows': len(df),
            'columns': list(df.columns),
            'memory_usage': df.memory_usage(deep=True).sum() / 1024**2,  # MB
            'null_counts': df.isnull().sum().to_dict()
        }
        return stats
    
    def load_dataset(self, dataset_key: str) -> Dict:
        """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì²˜ë¦¬"""
        try:
            # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
            file_path = self.download_dataset(dataset_key)
            
            # ë°ì´í„° ë¡œë“œ
            raw_data = self.load_jsonl(file_path)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            processed_df = self.preprocess_data(raw_data, dataset_key)
            
            # í†µê³„ ê³„ì‚°
            statistics = self.calculate_statistics(processed_df, dataset_key)
            
            # ê²°ê³¼ ì €ì¥
            self.datasets[dataset_key] = {
                'data': processed_df,
                'statistics': statistics
            }
            
            return {
                'dataset_key': dataset_key,
                'data': processed_df,
                'statistics': statistics
            }
        
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({dataset_key}): {str(e)}")
            raise

def display_dataset_info(dataset_info: Dict) -> None:
    """ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥"""
    print("\n" + "="*50)
    print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_info['dataset_key']}")
    print(f"ğŸ“ ì„¤ëª…: {DataConfig.DATASETS[dataset_info['dataset_key']]['desc']}")
    
    print("\nğŸ“ˆ í†µê³„ ì •ë³´:")
    stats = dataset_info['statistics']
    print(f"- ì´ í–‰ ìˆ˜: {stats['total_rows']:,}")
    print(f"- ì»¬ëŸ¼: {', '.join(stats['columns'])}")
    print(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage']:.2f} MB")
    
    print("\nğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(dataset_info['data'].head())
    print("="*50)

def main():
    try:
        # í™˜ê²½ ì„¤ì •
        dotenv.load_dotenv()
        
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        loader = CustomDataLoader()
        
        # QA ë°ì´í„°ì…‹ë§Œ ì²˜ë¦¬ (ì—ëŸ¬ ì—†ëŠ” ë°ì´í„°ì…‹)
        dataset_key = 'qa'
        print(f"\nğŸ”„ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘: {dataset_key}")
        dataset_info = loader.load_dataset(dataset_key)
        display_dataset_info(dataset_info)
    
    except Exception as e:
        print(f"âŒ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì—ëŸ¬: {str(e)}")
        raise

if __name__ == "__main__":
    main() 